# -*- coding: utf-8 -*-
"""abusive-comment-classification

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VjR6sFfc2BKT27iE4Xpfp841FUzdn94R
"""

#segregating the comments based on 6 categories namely toxic, threat, insult, identity abuse, severe toxic and obscene
import pandas as pd
import numpy as np

path = "https://raw.githubusercontent.com/schumanzhang/toxic_comment_classification/master/data/train.csv"
raw_data = pd.read_csv(path)
raw_data

print(raw_data.iloc[0,1], raw_data.iloc[1,1], raw_data.iloc[2,1])
obscene_data = raw_data[raw_data['obscene']==1]
display(obscene_data.head())

print(obscene_data.iloc[11,1])

# Commented out IPython magic to ensure Python compatibility.
import nltk
import sklearn
import gensim
from nltk import word_tokenize, sent_tokenize
from gensim.models.word2vec import Word2Vec
from sklearn.manifold import TSNE
from nltk.corpus import stopwords
import matplotlib.pyplot as plt                        
# %matplotlib inline

class vectorizer(object):
    def __init__(self, vector_method, size, sg, window, min_count, seed, workers, sents):
        self._vector_method = vector_method
        self._size = size
        self._sg = sg
        self._window = window
        self._min_count = min_count
        self._seed = seed
        self._sents = sents
        self._workers = workers
        
    def vectorize_model(self):
        model = self._vector_method(sentences=self._sents, size=self._size, sg=self._sg,
                 window=self._window, min_count=self._min_count, seed=self._seed,
                 workers=self._workers)
        
        return model

import nltk
nltk.download('punkt')

import nltk
nltk.download('stopwords')

# print(raw_data['comment_text'][:2])
def parse_sentences():
    sentences = []
    for index, row in raw_data.iterrows():
        text = row['comment_text']
        sent = list(sent_tokenize(text.lower()))
        sentences.append(sent)

    return sentences

def word_process(some_text):
    stop_words = set(stopwords.words('english'))    
    words = word_tokenize(some_text[0].lower())
    
    filtered_words = []
    for w in words:
        if w not in stop_words:
            filtered_words.append(w)
            
    return list(words) 

def parse_raw():
    documents = []
    sents = parse_sentences()
    for sent in sents:
        processed = word_process(sent)
        documents.append(processed)
    
    return documents

sentences = parse_raw()
# print(sentences)

word2vec_method = vectorizer(Word2Vec, 64, 1, 10, 3, 42, 2, sentences)
word2vec_model = word2vec_method.vectorize_model()
print('done processing')

print(word2vec_model.most_similar('fuck'))

tsne = TSNE(n_components=2, n_iter=300)

X = word2vec_model[word2vec_model.wv.vocab]
X_2d = tsne.fit_transform(X)
coords_df = pd.DataFrame(X_2d, columns=['x', 'y'])
coords_df['token'] = word2vec_model.wv.vocab.keys()

print(coords_df.head())

# Plot the graph.
coords_df.plot.scatter('x', 'y', figsize=(8,8),
                       marker='o', s=10, alpha=0.2)

# clean up the docs and labels a bit
all_docs = list(raw_data['comment_text'])
all_labels = np.array(list(raw_data['toxic']))

def retrieve_toxic_data():
    toxic_docs = []
    toxic_labels = []
    for index, row in raw_data.iterrows():
        if int(row['toxic']) == 1:
            toxic_docs.append(row['comment_text'])
            toxic_labels.append(raw_data.iloc[index,3:])
            
    return toxic_docs, np.array(toxic_labels), len(toxic_docs)

toxic_docs, toxic_labels, toxic_entries = retrieve_toxic_data()    

print(toxic_docs[:2])
print(toxic_labels[:6])
print(toxic_entries)

import keras
import pickle
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Embedding, Dropout, Dense, Flatten
from keras.callbacks import ModelCheckpoint
from keras.optimizers import SGD
feed_forw_network = 'https://github.com/schumanzhang/toxic_comment_classification/blob/master/notebooks/pickles/feed_forward_network'

class neural_network(object):
    def __init__(self, docs, labels, epochs, batch_size, dropout):
        self._model = Sequential()
        self._docs = docs
        self._labels = labels
        self._epochs = epochs
        self._batch_size = batch_size
        self._dropout = dropout
     
    def split_data(self, docs, targets):
        length = len(docs)
        split_point = int(round(length * 0.8))
        return docs[:split_point], targets[:split_point], docs[split_point:], targets[split_point:]
        
    def prepare_docs(self, max_length):
        t = Tokenizer(num_words=5000)
        t.fit_on_texts(self._docs)
        vocab_size = len(t.word_index) + 1
        encoded_docs = t.texts_to_sequences(self._docs)
        padded_docs = pad_sequences(encoded_docs, maxlen=max_length, padding='post')
        return padded_docs, t
    
    # only put in top 5000 words?
    def calc_embedding_matrix(self, vocab_size, t):
        embedding_matrix = np.zeros((vocab_size, 64))
        for word, i in t.word_index.items():
            if word in word2vec_model.wv.vocab:
                embedding_vector = word2vec_model.wv[word]
                if embedding_vector is not None:
                    embedding_matrix[i] = embedding_vector
                
        return embedding_matrix
    
    def construct_model(self, vocab_size, max_length, embedding_matrix):
        
        embedding_layer = Embedding(vocab_size, 64, weights=[embedding_matrix], input_length=max_length)
        embedding_layer.trainable = False
        
        self._model.add(embedding_layer)
        self._model.add(Flatten())
        self._model.add(Dense(64, activation='relu'))
        self._model.add(Dense(64, activation='relu'))
        self._model.add(Dropout(self._dropout))
        self._model.add(Dense(1, activation='sigmoid'))
        self._model.summary()
        
    def compile_model(self):
        sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)
        self._model.compile(optimizer=sgd, loss='binary_crossentropy', metrics=['accuracy'])
        checkpointer = ModelCheckpoint(filepath='saved_models/weights.feed_forward_network.hdf5', 
                               verbose=1, save_best_only=True)
        
        return checkpointer
        
    def train_save_model(self, train_docs, train_targets, valid_docs, valid_targets, checkpointer):
        trained_model = self._model.fit(train_docs, train_targets, validation_data=(valid_docs, valid_targets), 
                          epochs=self._epochs, batch_size=self._batch_size, callbacks=[checkpointer], verbose=1)

        with open(feed_forw_network, 'wb') as file_pi:
            pickle.dump(trained_model.history, file_pi)
            
        return trained_model
            
    def get_model(self):
        return self._model

feed_forward_network = neural_network(all_docs, all_labels, 8, 32, 0.5)
padded_docs, t = feed_forward_network.prepare_docs(150)

embedding_matrix = feed_forward_network.calc_embedding_matrix(len(t.word_index) + 1, t)
# print(embedding_matrix)

train_valid_docs, train_valid_targets, test_docs, test_targets = feed_forward_network.split_data(padded_docs, all_labels)
train_docs, train_targets, valid_docs, valid_targets = feed_forward_network.split_data(train_valid_docs, train_valid_targets)

feed_forward_network.construct_model(len(t.word_index) + 1, 150, embedding_matrix)

checkpointer = feed_forward_network.compile_model()

trained_feed_forward_model = feed_forward_network.train_save_model(train_docs, train_targets, valid_docs, valid_targets, checkpointer)

from sklearn.metrics import f1_score
from sklearn.metrics import precision_score
from sklearn.metrics import recall_score
from sklearn.metrics import fbeta_score

# test_docs, test_targets

class predictive_evaluate(object):
    def __init__(self, test_docs, test_targets, model):
        self._test_docs = test_docs
        self._test_targets = test_targets
        self._model = model
        
    def get_predictions(self):
        predictions = []
        for doc in self._test_docs:
            result = self._model.predict(np.expand_dims(doc, axis=0))
            preds = result[0][0]
            predictions.append(1 if preds >= 0.5 else 0)
                    
        return predictions
    
    def get_multilabel_predictions(self):
        predictions = []
        for doc in self._test_docs:
            result = self._model.predict(np.expand_dims(doc, axis=0))
            preds = result[0]
            preds[preds >= 0.5] = 1
            preds[preds < 0.5] = 0 
            predictions.append(preds)
                    
        return predictions
        
    def get_metrics(self, predictions):
        # print(self._test_targets[0])
        # print(predictions[0])
        model_recall_score = recall_score(self._test_targets, predictions, average='weighted')
        model_precision_score = precision_score(self._test_targets, predictions, average='weighted')
        model_f1_score = f1_score(self._test_targets, predictions, average='weighted')
        
        return model_recall_score, model_precision_score, model_f1_score

model = feed_forward_network.get_model()
model.load_weights('saved_models/weights.feed_forward_network.hdf5')

evaluation = predictive_evaluate(test_docs, test_targets, model)
predictions = np.array(evaluation.get_predictions()).astype(int)

print(predictions[100:110])

model_recall_score = recall_score(np.array(test_targets), predictions, average='weighted')
model_precision_score = precision_score(np.array(test_targets), predictions, average='weighted')
model_fbeta_score = fbeta_score(np.array(test_targets), predictions, average='weighted', beta=1)

print('Recall score:', model_recall_score)
print('Precision score:', model_precision_score)
print('F-beta score:', model_fbeta_score)

def occurence_of_labels(label_pos, labels):
    occurence = 0
    for label in labels:
        if int(label[label_pos]) == 1:
            occurence += 1
    
    return occurence / len(labels), occurence

frequency, total = occurence_of_labels(0, toxic_labels)
print(frequency, total)
frequency, total = occurence_of_labels(1, toxic_labels)
print(frequency, total)
frequency, total = occurence_of_labels(2, toxic_labels)
print(frequency, total)
frequency, total = occurence_of_labels(3, toxic_labels)
print(frequency, total)
frequency, total = occurence_of_labels(4, toxic_labels)
print(frequency, total)

from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation
from keras.layers import Bidirectional, GlobalMaxPool1D
from keras.models import Model
from keras import initializers, regularizers, constraints, optimizers, layers

list_classes = ["toxic", "severe_toxic", "obscene", "threat", "insult", "identity_hate"]

list_labels = raw_data[list_classes].values
list_docs = raw_data["comment_text"]

print(list_labels[:2])
print(list_docs[:2])

max_features = 20000
tokenizer = Tokenizer(num_words=max_features)
tokenizer.fit_on_texts(list(list_docs))
list_tokenized_docs = tokenizer.texts_to_sequences(list_docs)

print(list_tokenized_docs[:2])

# Use padding for consistency in length
maxlen = 200
X_t = pad_sequences(list_tokenized_docs, maxlen=maxlen)

print(X_t[:2])

# Have a look at the distribution of number of words in each doc
totalNumWords = [len(one_comment) for one_comment in list_tokenized_docs]
plt.hist(totalNumWords, bins = np.arange(0,410,10))
plt.show()

def split_data(docs, targets):
        length = len(docs)
        split_point = int(round(length * 0.8))
        return docs[:split_point], targets[:split_point], docs[split_point:], targets[split_point:]
    
train_valid_docs, train_valid_targets, test_docs, test_targets = split_data(X_t, list_labels)
train_docs, train_targets, valid_docs, valid_targets = split_data(train_valid_docs, train_valid_targets)

# construct the LSTM network here

inp = Input(shape=(maxlen, )) #maxlen=200 as defined earlier
embed_size = 128
x = Embedding(max_features, embed_size)(inp)
x = LSTM(60, return_sequences=True,name='lstm_layer')(x)
x = GlobalMaxPool1D()(x)
x = Dropout(0.1)(x)
x = Dense(50, activation="relu")(x)
x = Dropout(0.1)(x)
x = Dense(6, activation="sigmoid")(x)
model = Model(inputs=inp, outputs=x)
model.summary()

batch_size = 32
epochs = 2

model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])
checkpointer = ModelCheckpoint(filepath='saved_models/LSTM.hdf5', verbose=1, save_best_only=True)

model.fit(train_docs, train_targets, validation_data=(valid_docs, valid_targets), 
        epochs=epochs, batch_size=batch_size, callbacks=[checkpointer], verbose=1)

model.load_weights('saved_models/LSTM.hdf5')

evaluation = predictive_evaluate(test_docs, test_targets, model)
print('checkpoint')
print(len(test_docs))
print(len(test_targets))
predictions = np.array(evaluation.get_multilabel_predictions()).astype(int)

print(predictions[200:230])

model_recall_score = recall_score(np.array(test_targets), predictions, average=None)
model_precision_score = precision_score(np.array(test_targets), predictions, average=None)
model_fbeta_score = fbeta_score(np.array(test_targets), predictions, average='weighted', beta=1)

print(model_recall_score)
print(model_precision_score)
print(model_fbeta_score)
